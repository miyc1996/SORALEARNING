{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da04088c",
   "metadata": {},
   "source": [
    "#### AIGC技术基础知识-Stable Diffusion\n",
    "##### AIGC是什么\n",
    "##### AIGC技术的发展  \n",
    "##### 使用AIGC模型以及优化AIGC生成效果  \n",
    "##### 更多基于Stablediffusion的小应用  \n",
    "##### 视频生成技术发展"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e99f53b",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "- 你怎么理解Attention？  \n",
    "Attention（注意力机制）是一种在处理序列数据时的机制，它能够使得模型在处理每一个元素时能够“关注”到序列中的其他元素，并相应地调整自己的权重。在自然语言处理（NLP）中，注意力机制能够使得模型在处理每一个单词时能够关注到其他单词，从而更好地理解整个句子的含义。    \n",
    "- 乘性Attention和加性Attention有什么不同？  \n",
    "乘性Attention和加性Attention的区别主要在于计算注意力权重的方式不同。乘性Attention通过计算query和key的点积然后除以scaled factor来计算注意力权重，而加性Attention通过计算query和key的点积然后加上一个偏置项来计算注意力权重。  \n",
    "- Self-Attention为什么采用 Dot-Product Attention？  \n",
    "因为Dot-Product Attention在计算注意力权重时，通过计算query和key的点积然后除以scaled factor来得到注意力权重，这种方法简单且高效，能够使得模型在处理每一个元素时能够关注到序列中的其他元素，从而更好地进行序列处理任务。\n",
    "- Self-Attention中的Scaled因子有什么作用？必须是 sqrt(d_k) 吗？   \n",
    "Scaled factor的作用主要是为了使得注意力权重能够在不同的scale下进行计算，防止在计算过程中出现梯度消失或爆炸的问题。在Dot-Product Attention中，scaled factor通常是隐藏dim（hidden dimension）的平方根，即scaled factor = sqrt(d_k)。\n",
    "    \n",
    "- Multi-Head Self-Attention，Multi越多越好吗，为什么？  \n",
    "不是的，Multi-Head Self-Attention中的Multi-Head指的是将注意力机制分成多个头，每个头计算注意力权重时使用不同的权重矩阵，最后将各个头的注意力权重进行 concatenate。Multi-Head Self-Attention能够使得模型能够关注到序列中的不同特征，提高模型的性能。但是，Multi-Head Self-Attention中的头数（num_heads）并不是越多越好，头数过多可能会导致计算复杂度增加，从而影响模型的性能。在设计模型架构时，需要根据具体任务和数据集来选择合适的头数。\n",
    "\n",
    "- Multi-Head Self-Attention，固定hidden_dim，你认为增加 head_dim （需要缩小 num_heads）和减少 head_dim 会对结果有什么影响？  \n",
    "增加head_dim和减少head_dim会对结果有什么影响？如果固定hidden_dim，增加head_dim意味着每个头处理的特征维度减小，而减少head_dim意味着每个头处理的特征维度增大。在某些情况下，增加head_dim可能会提高模型的性能，因为它可以使得模型能够关注到序列中的更细粒度的特征。然而，如果head_dim过大，可能会导致模型过拟合，从而影响模型的性能。因此，在设计模型架构时，需要根据具体任务和数据集来选择合适的head_dim。\n",
    "\n",
    "- 为什么我们一般需要对 Attention weights 应用Dropout？哪些地方一般需要Dropout？Dropout在推理时是怎么执行的？你怎么理解Dropout？  \n",
    "因为在训练过程中，对Attention weights应用Dropout可以使得模型在训练过程中随机“丢弃”一些注意力权重，从而使得模型更加健壮，防止过拟合。在推理时，Dropout不会被执行，因此不会对模型的性能产生影响。\n",
    "\n",
    "- Self-Attention的qkv初始化时，bias怎么设置，为什么？  \n",
    "在Self-Attention中，qkv（query、key、value）的初始化时，通常会设置一个偏置项，偏置项的作用主要是为了使得模型在训练过程中能够更快地收敛。偏置项的值通常设置为一个较小的正数，例如0.1。\n",
    "\n",
    "- 你还知道哪些变种的Attention？它们针对Vanilla实现做了哪些优化和改进？  \n",
    "除了Vanilla Attention之外，还有一些变种的Attention，例如：\n",
    "1. Scaled Dot-Product Attention：在计算注意力权重时，将query和key的点积除以scaled factor，从而使得注意力权重能够在不同的scale下进行计算。\n",
    "2. Additive Attention：在计算注意力权重时，计算query和key的点积然后加上一个偏置项，从而得到注意力权重。\n",
    "3. Location-Based Attention：在计算注意力权重时，引入一个位置编码，使得模型能够更好地关注到序列中的特定位置的元素。  \n",
    "这些变种主要是针对Vanilla Attention的一些缺点和不足进行优化和改进，例如：  \n",
    "1. Scaled Dot-Product Attention：解决了在计算注意力权重时可能出现的梯度消失或爆炸问题。\n",
    "2. Additive Attention：解决了在计算注意力权重时可能出现的梯度消失或爆炸问题，同时提高了模型的性能。\n",
    "3. Location-Based Attention：解决了在处理序列数据时，模型无法很好地处理序列中的位置信息的问题。\n",
    "\n",
    "- 你认为Attention的缺点和不足是什么？ \n",
    "1. 计算复杂度较高：Attention机制在计算注意力权重时需要进行复杂的矩阵运算，这可能会导致计算复杂度较高，从而影响模型的性能。\n",
    "2. 可能过拟合：如果Attention机制的参数过多，可能会导致模型过拟合，从而影响模型的性能。\n",
    "3. 无法处理序列数据：Attention机制主要关注的是序列中的每一个元素，而无法处理序列数据中的顺序信息。\n",
    "\n",
    "- 你怎么理解Deep Learning的Deep？现在代码里只有一个Attention，多叠加几个效果会好吗？  \n",
    "Deep Learning的Deep是指在模型中包含多层神经网络，通过多层神经网络对数据进行抽象和特征提取，从而提高模型的性能。在代码中叠加多个Attention，可以使得模型能够更好地关注到序列中的不同特征，提高模型的性能。但是，如果Attention层数过多，可能会导致模型过拟合，从而影响模型的性能。因此，在设计模型架构时，需要根据具体任务和数据集来选择合适的Attention层数。\n",
    "\n",
    "- DeepLearning中Deep和Wide分别有什么作用，设计模型架构时应怎么考虑？  \n",
    "在DeepLearning中，Deep和Wide分别指深度学习和广度学习。广度学习主要关注的是模型在特征空间中的覆盖范围，而深度学习主要关注的是模型在特征空间中的抽象能力。在设计模型架构时，需要根据具体任务和数据集来选择合适的模型结构，从而达到更好的性能。\n",
    "\n",
    "#### LLM\n",
    "- 你怎么理解Tokenize？你知道几种Tokenize方式，它们有什么区别？    \n",
    "Tokenize方式主要有以下几种：  \n",
    "1. 基于规则的Tokenize：根据预定义的规则将文本分成单词或字符片段。这种Tokenize方式简单快速，但可能会受到文本中特殊字符或标点的影响。  \n",
    "2. 基于词典的Tokenize：使用词典将文本中的单词映射到对应的ID。这种Tokenize方式可以处理各种字符和标点，但需要预先构建一个完整的词典。  \n",
    "3. 基于模型的Tokenize：使用深度学习模型对文本进行分词。这种Tokenize方式可以自动学习文本中的分词规则，不需要预先构建词典，但可能需要更多的计算资源和训练时间。\n",
    "\n",
    "- 你觉得一个理想的Tokenizer模型应该具备哪些特点？  \n",
    "准确性：能够准确地分词，减少错误率。  \n",
    "高效性：分词速度快，能够满足实时应用的需求。  \n",
    "灵活性：能够处理各种文本格式和语言。  \n",
    "可扩展性：能够方便地扩展词典或调整分词规则。\n",
    "\n",
    "- Tokenizer中有一些特殊Token，比如开始和结束标记，你觉得它们的作用是什么？我们为什么不能通过模型自动学习到开始和结束标记？  \n",
    "我们不能通过模型自动学习到开始和结束标记，因为它们是预定义的，而不是从数据中学习到的。在训练过程中，模型需要学习到如何使用开始和结束标记来表示文本序列的上下文信息，从而提高模型的性能。\n",
    "\n",
    "- 为什么LLM都是Decoder-Only的？  \n",
    "因为在自然语言处理中，通常需要对输入序列进行编码，然后对编码进行解码以生成目标序列。在解码过程中，需要使用一个解码器（Decoder）来根据编码器（Encoder）的输出生成目标序列。由于解码器只需要根据编码器的输出进行生成，而不需要处理输入序列，因此可以实现更快的推理速度。\n",
    "\n",
    "- RMSNorm的作用是什么，和LayerNorm有什么不同？为什么不用LayerNorm？  \n",
    "RMSNorm的作用是对输入数据进行归一化处理，使得输入数据的均值为0，方差为1。与LayerNorm相比，RMSNorm在处理长序列时具有更好的性能，因为它能够更好地适应不同长度的序列。然而，RMSNorm在训练过程中可能会受到梯度消失的影响，因此通常在训练过程中使用LayerNorm，而在推理过程中使用RMSNorm。\n",
    "\n",
    "- LLM中的残差连接体现在哪里？为什么用残差连接？  \n",
    "在LLM中，残差连接体现在模型的每一层中。残差连接是指在模型层之间添加一个跳跃连接，使得模型可以在层之间传递信息，提高模型的性能。使用残差连接可以解决梯度消失问题，提高模型的训练速度和性能。\n",
    "\n",
    "- PreNormalization和PostNormalization会对模型有什么影响？为什么现在LLM都用PreNormalization？  \n",
    "PreNormalization和PostNormalization会对模型产生不同的影响。PreNormalization在模型层之间进行归一化处理，使得输入数据的均值为0，方差为1。这种归一化方式可以使得模型在训练过程中更加稳定，提高模型的性能。PostNormalization在模型层之后进行归一化处理，这种归一化方式可以使得模型在训练过程中更加灵活，但可能会导致梯度消失问题。现在LLM都用PreNormalization的原因是，PreNormalization在训练过程中可以更好地适应不同长度的序列，提高模型的性能。而PostNormalization在训练过程中可能会受到梯度消失的影响，因此通常在推理过程中使用PostNormalization。\n",
    "\n",
    "- FFN为什么先扩大后缩小，它们的作用分别是什么？  \n",
    "在FFN中，首先通过一个线性层将输入特征映射到更大的空间，然后使用激活函数（如ReLU）进行非线性转换，最后通过另一个线性层将特征映射回原始空间。这种结构可以使得模型能够更好地学习输入特征之间的交互关系，提高模型的性能。\n",
    "- 为什么LLM需要位置编码？你了解几种位置编码方案？  \n",
    "在自然语言处理中，位置编码是指将文本序列中的每个位置编码为一个向量，用于表示该位置的语义信息。在LLM中，位置编码可以帮助模型更好地处理序列中的位置信息，提高模型的性能。  \n",
    "位置编码方案主要有以下几种:  \n",
    "1. 基于规则的位置编码：根据预定义的规则为每个位置编码一个固定的向量。这种位置编码方式简单快速，但可能会受到文本中特殊字符或标点的影响。 \n",
    "2. 基于词典的位置编码：使用词典将文本中的单词映射到对应的位置编码向量。这种位置编码方式可以处理各种字符和标点，但需要预先构建一个完整的词典。  \n",
    "3. 基于模型的位置编码：使用深度学习模型对文本进行分词，然后为每个分词分配一个位置编码向量。这种位置编码方式可以自动学习文本中的位置信息，不需要预先构建词典，但可能需要更多的计算资源和训练时间。\n",
    "- 为什么RoPE能从众多位置编码中脱颖而出？它主要做了哪些改进？  \n",
    "RoPE（Relative Position Encoding）是一种基于位置编码的改进方案，它通过计算相对位置编码向量来表示文本序列中的位置信息。RoPE在处理长序列时具有更好的性能，因为它能够更好地适应不同长度的序列。此外，RoPE在训练过程中可以自动学习位置信息，而不需要预先构建词典，因此可以提高模型的性能。  \n",
    "RoPE的主要改进包括：  \n",
    "使用相对位置编码向量来表示位置信息，而不是使用绝对位置编码向量。  \n",
    "在计算相对位置编码向量时，使用了一个双线性插值函数来提高位置编码的准确性。  \n",
    "在训练过程中，RoPE会自动学习位置信息，而不需要预先构建词典。\n",
    "- 如果让你设计一种位置编码方案，你会考虑哪些因素？  \n",
    "还未考虑\n",
    "- 请你将《LLM部分》中的一些设计（如RMSNorm）加入到《Self-Attention部分》的模型设计中，看看能否提升效果？  \n",
    "暂时未做到这一步"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06eeb11",
   "metadata": {},
   "source": [
    "#### 基于Transformers，diffusion技术解析+实战\n",
    "1. Transformers+diffusion技术背景简介\n",
    "2. UViT读论文\n",
    "3. 代码实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0f527",
   "metadata": {},
   "source": [
    "#### 声音生成TTS技术解析与实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1654d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
